#!/usr/bin/env bash

#1) If site-update already running, kill it through lock file (kill -15 wait, check kill -9 wait)
#2) Get lock
#3) Backup ghost directory to S3 backup bucket
#4) generate static site
#5) Sync static site to S3 www bucket
#6) Invalidate CloudFront

set -a; source $(dirname $0)/../.env; set +a

# Backup content images
aws s3 sync --delete --no-follow-symlinks --no-progress $GHOST_DIR/content/images $CMS_BUCKET_PREFIX/images

if ! (cd $GHOST_DIR; ghost status | grep running >/dev/null)
then
  echo "ERROR: aborting ghost  stopped"
  exit
fi

echo "*** pulling down website"
gssg --domain https://${CMS_HOSTNAME} --url https://${WEB_HOSTNAME}  --dest $WEB_DIR

# TODO move this into gssg
if [ "$GHOST_API_KEY" != "" ]
then
  mkdir -p $WEB_DIR/files
  echo "download posts for ghostHunter"
  curl "https://${CMS_HOSTNAME}/ghost/api/v2/content/posts/?key=${GHOST_API_KEY}&limit=all&include=tags&formats=plaintext" |
    perl -pe "s/${CMS_HOSTNAME}/${WEB_HOSTNAME}/g" > $WEB_DIR/files/posts.json
  echo '{"latestPost": "'$(date  --iso-8601=seconds)'"}' > $WEB_DIR/files/posts.latest.json
fi

echo "*** syncing to web site bucket"
aws s3 sync --delete --no-progress $WEB_DIR $WEB_BUCKET_PREFIX
echo "*** invalidating cloudfront"

# Upload public files
for DIR in $(find $GHOST_DIR/content -name public_files)
do
  aws s3 sync --no-progress $DIR $WEB_BUCKET_PREFIX/files
done

aws cloudfront create-invalidation --distribution-id $CLOUDFRONT_ID --paths '/*'
echo "*** all done publishing"
